{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Retrieve an arbitrary Wikipedia page of \"Python\" and create a list of links on that page: `url ='https://en.wikipedia.org/wiki/Python'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "url ='https://en.wikipedia.org/wiki/Python_(programming_language)'\n",
    "\n",
    "response = requests.get(url)\n",
    "response.status_code\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "reflist = soup.select('span.reference-text > cite > a:nth-child(1)')\n",
    "\n",
    "links = []\n",
    "\n",
    "for i in reflist:\n",
    "    str_url = str(i['href'])\n",
    "    if str_url[:4] != 'http':\n",
    "        str_url = 'https://en.wikipedia.org' + str_url\n",
    "    links.append(str_url)\n",
    "\n",
    "#links ##uncomment to see the list of links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Find the number of titles that have changed in the United States Code since its last release point: `url = 'http://uscode.house.gov/download/download.shtml'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changed titles:\n",
      "Title 1 - General Provisions٭\n",
      "Title 2 - The Congress\n",
      "Title 5 - Government Organization and Employees٭\n",
      "Title 6 - Domestic Security\n",
      "Title 7 - Agriculture\n",
      "Title 12 - Banks and Banking\n",
      "Title 15 - Commerce and Trade\n",
      "Title 16 - Conservation\n",
      "Title 19 - Customs Duties\n",
      "Title 23 - Highways٭\n",
      "Title 25 - Indians\n",
      "Title 26 - Internal Revenue Code\n",
      "Title 29 - Labor\n",
      "Title 30 - Mineral Lands and Mining\n",
      "Title 33 - Navigation and Navigable Waters\n",
      "Title 40 - Public Buildings, Property, and Works٭\n",
      "Title 41 - Public Contracts٭\n",
      "Title 42 - The Public Health and Welfare\n",
      "Title 43 - Public Lands\n",
      "Title 45 - Railroads\n",
      "Title 46 - Shipping٭\n",
      "Title 47 - Telecommunications\n",
      "Title 49 - Transportation٭\n",
      "Title 54 - National Park Service and Related Programs٭\n",
      "Total changed:  24\n"
     ]
    }
   ],
   "source": [
    "url ='http://uscode.house.gov/download/download.shtml'\n",
    "\n",
    "response = requests.get(url)\n",
    "response.status_code\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "changed = soup.find_all('div', {'class':'usctitlechanged'})\n",
    "\n",
    "print('Changed titles:')\n",
    "\n",
    "for i in changed:\n",
    "    print(i.get_text(strip=True))\n",
    "\n",
    "print('Total changed: ', len(changed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create a Python list with the top ten FBI's Most Wanted names: `url = 'https://www.fbi.gov/wanted/topten'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['JASON DEREK BROWN', 'ALEXIS FLORES', 'JOSE RODOLFO VILLARREAL-HERNANDEZ', 'RAFAEL CARO-QUINTERO', 'YULAN ADONAY ARCHAGA CARIAS', 'EUGENE PALMER', 'BHADRESHKUMAR CHETANBHAI PATEL', 'ALEJANDRO ROSALES CASTILLO', 'ARNOLDO JIMENEZ', 'OCTAVIANO JUAREZ-CORRO']\n"
     ]
    }
   ],
   "source": [
    "url ='https://www.fbi.gov/wanted/topten'\n",
    "\n",
    "response = requests.get(url)\n",
    "response.status_code\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "\n",
    "# Copy CSS selector for the element from the page:\n",
    "# (li.portal-type-person:nth-child(1) > h3:nth-child(2) > a:nth-child(1))\n",
    "# and remove \":nth-child(1)\" to select all \"li.portal-type-person\" items.\n",
    "\n",
    "most_wanted = soup.select('li.portal-type-person > h3:nth-child(2) > a:nth-child(1)')\n",
    "\n",
    "most_wanted_list = []\n",
    "\n",
    "for i in most_wanted:\n",
    "    most_wanted_list.append(i.get_text(strip=True))\n",
    "\n",
    "print(most_wanted_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Display the 20 latest earthquakes info (date, time, latitude, longitude and region name) by the EMSC as a pandas dataframe: `url = 'https://www.emsc-csem.org/Earthquake/'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>region_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-03-02</td>\n",
       "      <td>18:52:47.7</td>\n",
       "      <td>35.69 N</td>\n",
       "      <td>121.13 W</td>\n",
       "      <td>CENTRAL CALIFORNIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-03-02</td>\n",
       "      <td>18:03:42.0</td>\n",
       "      <td>13.94 N</td>\n",
       "      <td>124.29 E</td>\n",
       "      <td>CATANDUANES, PHILIPPINES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-03-02</td>\n",
       "      <td>17:37:43.0</td>\n",
       "      <td>0.19 N</td>\n",
       "      <td>100.01 E</td>\n",
       "      <td>NORTHERN SUMATRA, INDONESIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-03-02</td>\n",
       "      <td>17:32:58.0</td>\n",
       "      <td>0.21 N</td>\n",
       "      <td>100.07 E</td>\n",
       "      <td>NORTHERN SUMATRA, INDONESIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-03-02</td>\n",
       "      <td>17:03:51.0</td>\n",
       "      <td>19.22 N</td>\n",
       "      <td>121.32 E</td>\n",
       "      <td>BABUYAN ISL REGION, PHILIPPINES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2022-03-02</td>\n",
       "      <td>17:02:34.1</td>\n",
       "      <td>49.33 N</td>\n",
       "      <td>155.92 E</td>\n",
       "      <td>KURIL ISLANDS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2022-03-02</td>\n",
       "      <td>16:57:48.3</td>\n",
       "      <td>35.21 N</td>\n",
       "      <td>25.32 E</td>\n",
       "      <td>CRETE, GREECE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2022-03-02</td>\n",
       "      <td>16:56:06.6</td>\n",
       "      <td>39.24 N</td>\n",
       "      <td>20.59 E</td>\n",
       "      <td>GREECE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2022-03-02</td>\n",
       "      <td>16:45:40.0</td>\n",
       "      <td>36.37 N</td>\n",
       "      <td>8.92 W</td>\n",
       "      <td>WEST OF GIBRALTAR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2022-03-02</td>\n",
       "      <td>16:37:50.2</td>\n",
       "      <td>40.07 N</td>\n",
       "      <td>22.32 E</td>\n",
       "      <td>GREECE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2022-03-02</td>\n",
       "      <td>16:32:15.5</td>\n",
       "      <td>43.49 S</td>\n",
       "      <td>172.77 E</td>\n",
       "      <td>SOUTH ISLAND OF NEW ZEALAND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2022-03-02</td>\n",
       "      <td>16:20:24.2</td>\n",
       "      <td>18.35 N</td>\n",
       "      <td>66.09 W</td>\n",
       "      <td>SAN JUAN URBAN AREA, PUERTO RICO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2022-03-02</td>\n",
       "      <td>16:15:41.0</td>\n",
       "      <td>24.24 S</td>\n",
       "      <td>67.10 W</td>\n",
       "      <td>SALTA, ARGENTINA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2022-03-02</td>\n",
       "      <td>16:08:13.0</td>\n",
       "      <td>17.09 N</td>\n",
       "      <td>93.38 W</td>\n",
       "      <td>CHIAPAS, MEXICO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2022-03-02</td>\n",
       "      <td>16:08:01.9</td>\n",
       "      <td>37.83 N</td>\n",
       "      <td>26.51 E</td>\n",
       "      <td>DODECANESE ISLANDS, GREECE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2022-03-02</td>\n",
       "      <td>15:56:30.0</td>\n",
       "      <td>10.62 N</td>\n",
       "      <td>84.97 W</td>\n",
       "      <td>COSTA RICA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2022-03-02</td>\n",
       "      <td>15:46:01.0</td>\n",
       "      <td>14.52 N</td>\n",
       "      <td>92.61 W</td>\n",
       "      <td>OFFSHORE CHIAPAS, MEXICO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2022-03-02</td>\n",
       "      <td>15:33:43.2</td>\n",
       "      <td>35.60 N</td>\n",
       "      <td>3.60 W</td>\n",
       "      <td>STRAIT OF GIBRALTAR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2022-03-02</td>\n",
       "      <td>15:24:29.9</td>\n",
       "      <td>18.04 N</td>\n",
       "      <td>65.43 W</td>\n",
       "      <td>PUERTO RICO REGION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2022-03-02</td>\n",
       "      <td>15:09:32.0</td>\n",
       "      <td>40.73 N</td>\n",
       "      <td>27.42 E</td>\n",
       "      <td>WESTERN TURKEY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          date        time latitude longitude  \\\n",
       "0   2022-03-02  18:52:47.7  35.69 N  121.13 W   \n",
       "1   2022-03-02  18:03:42.0  13.94 N  124.29 E   \n",
       "2   2022-03-02  17:37:43.0   0.19 N  100.01 E   \n",
       "3   2022-03-02  17:32:58.0   0.21 N  100.07 E   \n",
       "4   2022-03-02  17:03:51.0  19.22 N  121.32 E   \n",
       "5   2022-03-02  17:02:34.1  49.33 N  155.92 E   \n",
       "6   2022-03-02  16:57:48.3  35.21 N   25.32 E   \n",
       "7   2022-03-02  16:56:06.6  39.24 N   20.59 E   \n",
       "8   2022-03-02  16:45:40.0  36.37 N    8.92 W   \n",
       "9   2022-03-02  16:37:50.2  40.07 N   22.32 E   \n",
       "10  2022-03-02  16:32:15.5  43.49 S  172.77 E   \n",
       "11  2022-03-02  16:20:24.2  18.35 N   66.09 W   \n",
       "12  2022-03-02  16:15:41.0  24.24 S   67.10 W   \n",
       "13  2022-03-02  16:08:13.0  17.09 N   93.38 W   \n",
       "14  2022-03-02  16:08:01.9  37.83 N   26.51 E   \n",
       "15  2022-03-02  15:56:30.0  10.62 N   84.97 W   \n",
       "16  2022-03-02  15:46:01.0  14.52 N   92.61 W   \n",
       "17  2022-03-02  15:33:43.2  35.60 N    3.60 W   \n",
       "18  2022-03-02  15:24:29.9  18.04 N   65.43 W   \n",
       "19  2022-03-02  15:09:32.0  40.73 N   27.42 E   \n",
       "\n",
       "                         region_name  \n",
       "0                 CENTRAL CALIFORNIA  \n",
       "1           CATANDUANES, PHILIPPINES  \n",
       "2        NORTHERN SUMATRA, INDONESIA  \n",
       "3        NORTHERN SUMATRA, INDONESIA  \n",
       "4    BABUYAN ISL REGION, PHILIPPINES  \n",
       "5                      KURIL ISLANDS  \n",
       "6                      CRETE, GREECE  \n",
       "7                             GREECE  \n",
       "8                  WEST OF GIBRALTAR  \n",
       "9                             GREECE  \n",
       "10       SOUTH ISLAND OF NEW ZEALAND  \n",
       "11  SAN JUAN URBAN AREA, PUERTO RICO  \n",
       "12                  SALTA, ARGENTINA  \n",
       "13                   CHIAPAS, MEXICO  \n",
       "14        DODECANESE ISLANDS, GREECE  \n",
       "15                        COSTA RICA  \n",
       "16          OFFSHORE CHIAPAS, MEXICO  \n",
       "17               STRAIT OF GIBRALTAR  \n",
       "18                PUERTO RICO REGION  \n",
       "19                    WESTERN TURKEY  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "url ='https://www.emsc-csem.org/Earthquake/'\n",
    "\n",
    "response = requests.get(url)\n",
    "response.status_code\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Select first 20 table rows:\n",
    "rows = soup.select('#tbody > tr')\n",
    "\n",
    "date = []\n",
    "time = []\n",
    "latitude = []\n",
    "longitude = []\n",
    "region_name = []\n",
    "\n",
    "# print(rows[0].find_all('td')[3])\n",
    "\n",
    "for row in rows:\n",
    "    # Find all columns in a row\n",
    "    columns = row.find_all('td')\n",
    "\n",
    "    # Find date and time (4th column), find \"a\" tag and get text. Then split date and time to the new list.\n",
    "    # Then append date to \"date\" list and time to \"time\" list.\n",
    "    date_time_list = columns[3].find('a').get_text(strip=True).split()\n",
    "    date.append(date_time_list[0])\n",
    "    time.append(date_time_list[1])\n",
    "\n",
    "    # Get latitude - join text in columns 5 and 6. Append to \"latitude\" list.\n",
    "    la1 = columns[4].get_text(strip=True)\n",
    "    la2 = columns[5].get_text(strip=True)\n",
    "    latitude.append(' '.join([la1, la2]))\n",
    "\n",
    "    # Get longitude - join text in columns 7 and 8. Append to \"longitude\" list.\n",
    "    lo1 = columns[6].get_text(strip=True)\n",
    "    lo2 = columns[7].get_text(strip=True)\n",
    "    longitude.append(' '.join([lo1, lo2]))\n",
    "\n",
    "    # Get region from column 12 and append to \"region_name\" list.\n",
    "    region_name.append(columns[11].get_text(strip=True))\n",
    "\n",
    "earthquakes = pd.DataFrame({'date':date, 'time':time, 'latitude':latitude, 'longitude':longitude, 'region_name':region_name})\n",
    "\n",
    "earthquakes.head(20)\n",
    "\n",
    "# print(date)\n",
    "# print(time)\n",
    "# print(latitude)\n",
    "# print(longitude)\n",
    "# print(region_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- List all language names and number of related articles in the order they appear in [wikipedia.org](wikipedia.org): `url = 'https://www.wikipedia.org/'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en - English - 6 383 000+\n",
      "ja - 日本語 - 1 292 000+\n",
      "ru - Русский - 1 756 000+\n",
      "de - Deutsch - 2 617 000+\n",
      "es - Español - 1 717 000+\n",
      "fr - Français - 2 362 000+\n",
      "zh - 中文 - 1 231 000+\n",
      "it - Italiano - 1 718 000+\n",
      "pl - Polski - 1 490 000+\n",
      "pt - Português - 1 074 000+\n"
     ]
    }
   ],
   "source": [
    "url ='https://www.wikipedia.org'\n",
    "\n",
    "response = requests.get(url)\n",
    "response.status_code\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "\n",
    "# Copy CSS selector for the element from the page:\n",
    "# (div.central-featured-lang:nth-child(1))\n",
    "# and remove \":nth-child(1)\" to select all \"div.central-featured-lang\" items.\n",
    "\n",
    "lang_list = soup.select('div.central-featured-lang')\n",
    "\n",
    "for i in lang_list:\n",
    "    # Find language code\n",
    "    code = i['lang']\n",
    "    # Find language name\n",
    "    name = i.find('strong').get_text()\n",
    "    # Find the number of articles\n",
    "    number = i.find('bdi').get_text()\n",
    "    \n",
    "    print(' - '.join([code, name, number]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A list with the different kind of datasets available in [data.gov.uk](data.gov.uk): `url = 'https://data.gov.uk/'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Business and economy', 'Crime and justice', 'Defence', 'Education', 'Environment', 'Government', 'Government spending', 'Health', 'Mapping', 'Society', 'Towns and cities', 'Transport', 'Digital service performance', 'Government reference data']\n"
     ]
    }
   ],
   "source": [
    "url = 'https://data.gov.uk'\n",
    "\n",
    "response = requests.get(url)\n",
    "response.status_code\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "\n",
    "# Copy CSS selector for the element from the page:\n",
    "# (.govuk-list > li:nth-child(1) > h3:nth-child(1) > a:nth-child(1))\n",
    "# and remove \":nth-child(1)\" from \"li:nth-child(1)\" to select all items.\n",
    "\n",
    "datasets = soup.select('.govuk-list > li > h3:nth-child(1) > a:nth-child(1)')\n",
    "\n",
    "dataset_list = []\n",
    "\n",
    "for i in datasets:\n",
    "    dataset_list.append(i.get_text(strip=True))\n",
    "\n",
    "print(dataset_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Display the top 10 languages by number of native speakers stored in a pandas dataframe: `url = 'https://en.wikipedia.org/wiki/List_of_languages_by_number_of_native_speakers'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mandarin Chinese</td>\n",
       "      <td>918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Spanish</td>\n",
       "      <td>480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>English</td>\n",
       "      <td>379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hindi(sanskritisedHindustani)[11]</td>\n",
       "      <td>341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bengali</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Portuguese</td>\n",
       "      <td>221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Russian</td>\n",
       "      <td>154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Japanese</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Western Punjabi[12]</td>\n",
       "      <td>92.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Marathi</td>\n",
       "      <td>83.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            language number\n",
       "0                   Mandarin Chinese    918\n",
       "1                            Spanish    480\n",
       "2                            English    379\n",
       "3  Hindi(sanskritisedHindustani)[11]    341\n",
       "4                            Bengali    300\n",
       "5                         Portuguese    221\n",
       "6                            Russian    154\n",
       "7                           Japanese    128\n",
       "8                Western Punjabi[12]   92.7\n",
       "9                            Marathi   83.1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://en.wikipedia.org/wiki/List_of_languages_by_number_of_native_speakers'\n",
    "\n",
    "response = requests.get(url)\n",
    "response.status_code\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Copy CSS selector for the element from the page:\n",
    "# (table.wikitable:nth-child(17) > tbody:nth-child(3) > tr:nth-child(1))\n",
    "# and remove \":nth-child(1)\" from \"tbody\" and \"tr\" to select all items.\n",
    "# Add \"not(:first-child)\" to \"tr\" to ignore header row.\n",
    "\n",
    "languages = soup.select('table.wikitable:nth-child(17) > tbody > tr:not(:first-child)')\n",
    "\n",
    "language = []\n",
    "number = []\n",
    "\n",
    "for i in languages:\n",
    "    # Find all columns in a row\n",
    "    columns = i.find_all('td')\n",
    "\n",
    "    # Get language from column 2 and append to \"language\" list.\n",
    "    language.append(columns[1].get_text(strip=True))\n",
    "\n",
    "    # Get the number of native speakers from column 3 and append to \"number\" list.\n",
    "    number.append(columns[2].get_text(strip=True))\n",
    "\n",
    "languages_list = pd.DataFrame({'language':language, 'number':number})\n",
    "languages_list.head(10)\n",
    "\n",
    "# print(language)\n",
    "# print(number)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
